---
analyze_csv_data_template_for_patterns:
  description: |-
    Analyze the CSV data template provided in {csv_data_template} to identify data structure and patterns. The CSV contains headers and sample rows that serve as reference. Focus on understanding:

    1. **Column Structure**: Parse headers and understand field names and their purposes
    2. **Data Types**: Determine data types for each column (strings, integers, dates, coordinates, etc.)
    3. **Data Formats**: Recognize patterns like date formats, ID structures, naming conventions, coordinate formats
    4. **Value Analysis**: Understand value ranges, distributions, and constraints from sample data
    5. **Field Relationships**: Identify connections and dependencies between different columns
    6. **Business Logic**: Extract evident business rules and data validation patterns from samples
    7. **Pattern Recognition**: Identify naming patterns, sequential numbering, formatting rules

    Parse the provided CSV data from {csv_data_template} and provide comprehensive analysis for synthetic data generation.
  expected_output: |-
    A detailed data pattern analysis report in markdown format containing:

    **CSV Structure Analysis:**
    - Complete list of column headers and their data types
    - Sample data examples for each column
    - Field relationships and dependencies
    - Value patterns, ranges, and distributions

    **Data Format Specifications:**
    - Exact format requirements for each field type
    - Pattern examples (e.g., date formats, ID patterns, coordinate formats)
    - Naming conventions and structural rules
    - Constraints and validation requirements

    **Synthetic Generation Guidelines:**
    - Specific rules for generating realistic {num_rows} synthetic records
    - Business logic constraints to maintain
    - Format specifications to follow precisely
    - Value generation parameters and acceptable ranges
    - Inter-field relationship rules to preserve

    This analysis will serve as the comprehensive foundation for creating authentic synthetic data that matches the original template.
  agent: csv_data_pattern_analyst

generate_synthetic_data_rows:
  description: |-
    Based on the analyzed data patterns from the CSV template, generate {num_rows} new synthetic data rows that accurately mimic the original dataset characteristics. Ensure:

    1. Maintain exact same column structure and data types from the CSV template
    2. Follow identified naming conventions and ID patterns
    3. Respect value ranges and distributions from the sample data
    4. Create realistic relationships between fields (e.g., coordinates matching addresses)
    5. Apply business logic constraints identified in the analysis
    6. Generate diverse but realistic data that could plausibly exist in the same system
    7. Preserve formatting patterns and data consistency rules

    The synthetic data should be indistinguishable from real data in terms of format and patterns, suitable for testing or augmentation purposes.
  expected_output: |-
    A complete set of {num_rows} synthetic data rows in the exact same format as the original dataset. The output should include:
    - Properly formatted tabular data matching the original structure
    - Sequential or appropriate numbering/ID generation
    - Realistic and diverse data values
    - Consistent formatting and data types
    - Clear presentation in markdown table format or CSV-ready format
  agent: synthetic_data_generator
  context:
    - analyze_csv_data_template_for_patterns

validate_and_finalize_synthetic_dataset:
  description: |-
    Review and validate the generated synthetic data to ensure it meets quality standards and consistency with the original dataset patterns. Perform comprehensive quality checks including:

    1. Data format consistency validation
    2. Business logic compliance verification
    3. Pattern adherence confirmation
    4. Inter-field relationship validation
    5. Data type and range compliance
    6. Uniqueness and realism assessment
    7. Overall dataset coherence evaluation

    If any issues are identified, provide specific feedback for improvement. Ensure the final dataset is production-ready and maintains the same standards as the original data.
  expected_output: |-
    A final validation report containing:
    - Quality assessment summary with pass/fail status for each validation criterion
    - Any identified issues or inconsistencies with recommended fixes
    - Final approved synthetic dataset with {num_rows} rows in proper format
    - Data quality metrics and compliance confirmation
    - Ready-to-use dataset that seamlessly integrates with the original data standards
  agent: data_quality_orchestrator
  context:
    - analyze_csv_data_template_for_patterns
    - generate_synthetic_data_rows

store_synthetic_data_in_multiple_formats:
  description: |-
    Take the validated synthetic dataset and store it in two formats: CSV and JSON. Your tasks include:

    **1. CSV File Export:**
    - Generate a clean CSV file with proper headers and formatting
    - Ensure all {num_rows} synthetic records are included

    **2. JSON File Export:**
    - Create a structured JSON file for API integration and programmatic access
    - Ensure all {num_rows} synthetic records are included

    **3. Quality Assurance:**
    - Verify data consistency between CSV and JSON files
    - Validate successful storage and accessibility
    - Provide access instructions and file locations

    Ensure the same {num_rows} records are available in both formats with consistent data integrity.
  expected_output: |-
    A data package containing:

    **File Exports:**
    - **CSV File**: Clean, properly formatted CSV with headers
    - **JSON File**: Structured JSON format for API and programmatic use

    **Storage Summary:**
    - Storage location details for both files
    - File size metrics and record counts verification
    - Access instructions for each format
    - Data integrity validation report confirming consistency between CSV and JSON

    The synthetic dataset will be immediately usable for spreadsheet analysis and API integration.
  agent: multi_format_data_storage_specialist
  context:
    - validate_and_finalize_synthetic_dataset